{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GL7-l673sqYA",
        "outputId": "11c28ab4-3914-4494-9eb8-acccbce696c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 18331 images belonging to 10 classes.\n",
            "Found 7848 images belonging to 10 classes.\n",
            "Creating a new model...\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.2549 - loss: 2.1436 "
          ]
        }
      ],
      "source": [
        "# By Troy Waters (solo, no team members)\n",
        "# CECS 456 Final Project\n",
        "# Student ID : 029451303\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# All ~26,000 images are stored on my personal Google Drive\n",
        "# This is due to issues with obtaining the images directly\n",
        "# from the website, and this was the only consistent\n",
        "# way to get results on my low-end laptop\n",
        "dataset_path = '/content/drive/My Drive/archive/raw-img'\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# datagen will handle normalizing pixels\n",
        "# because PC's handle decimate calculations (0 to 1)\n",
        "# better than large integers ( such as 0 to 255)\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # normalize\n",
        "    validation_split=0.3  # Splitting for validation later\n",
        ")\n",
        "\n",
        "# Given Google collab RAM constraints\n",
        "# Images are resized to half their original size\n",
        "# And batches of 32 images are handled at a time\n",
        "# To avoid early termination\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(128, 128),  # 255x255 px to 128x128 px\n",
        "    batch_size=32, #32 images at a time\n",
        "    class_mode='categorical', # 10 classes of animals\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# validation set has identical\n",
        "# properties to the training set\n",
        "# to maintain consistency\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Creation of the Sequential CNN model\n",
        "\n",
        "def create_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D((2, 2)), # Shrinking feature map for optimization\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)), # Shrinking once more\n",
        "        Flatten(), # Turning 2D feature maps to 1D\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5), # To avoid overfittin\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 128x128 px, and 3 color channels in the image (Red, Green, Blue)\n",
        "input_shape = (128, 128, 3)\n",
        "num_classes = len(train_generator.class_indices)\n",
        "\n",
        "# Since google collab can terminate when you least expect it\n",
        "# 'check points' were added\n",
        "# as a safety measure, so we don't need to re-run 10 hours of lost time\n",
        "if os.path.exists('best_animal_classifier.keras'):\n",
        "    print(\"Loading saved model...\")\n",
        "    model = load_model('best_animal_classifier.keras')\n",
        "else:\n",
        "    print(\"Creating a new model...\")\n",
        "    model = create_model(input_shape, num_classes)\n",
        "\n",
        "# saving every epoch for sanity sake...\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_animal_classifier.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Start at epoch 0, but check if we have previous model\n",
        "initial_epoch = 0\n",
        "if os.path.exists('training_history.log'):\n",
        "    with open('training_history.log', 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        if lines:\n",
        "            initial_epoch = int(lines[-1].strip()) + 1\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator,\n",
        "    initial_epoch=initial_epoch,  # checkpoint would go here if it exists\n",
        "    callbacks=[checkpoint]\n",
        ")\n",
        "\n",
        "# Saving training\n",
        "with open('training_history.log', 'a') as f:\n",
        "    for epoch in range(initial_epoch, 20):\n",
        "        f.write(f\"{epoch}\\n\")\n",
        "\n",
        "# Evaluating the validation models for highest accuracy\n",
        "test_model = load_model('best_animal_classifier.keras')\n",
        "test_loss, test_acc = test_model.evaluate(val_generator)\n",
        "print(f\"Test Accuracy: {test_acc:.2f}\")\n",
        "\n",
        "# visualization of all the data once complete\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}